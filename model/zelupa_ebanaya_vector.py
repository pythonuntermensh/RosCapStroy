# -*- coding: utf-8 -*-
"""Zelupa Ebanaya Vector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ru-4eRUDEyNj6AWV48Zr59j9ZL8BBFE9
"""

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

import re
import pandas as pd
import numpy as np
from tqdm.auto import tqdm, trange

data = pd.read_excel("Датасет.xlsx")

df = pd.DataFrame()
df.loc[:, 'responsibilities'] = data['responsibilities(Должностные обязанности)']
df.loc[:, 'requirements'] = data['requirements(Требования к соискателю)']
df.loc[:, 'terms'] = data['terms(Условия)']
df.loc[:, 'notes'] = data['notes(Примечания)']

for i in range(df.shape[0]):
  df['responsibilities'][i] = df['responsibilities'][i].replace(str(df['requirements'][i]), "")
  df['responsibilities'][i] = df['responsibilities'][i].replace(str(df['terms'][i]), "")
  df['responsibilities'][i] = df['responsibilities'][i].replace(str(df['notes'][i]), "")

df[:10]

df_1 = pd.DataFrame(columns = ["Text", "Class", "Preproccessed"])


for x in df['responsibilities']:
  df_1 = df_1.append({"Text": x, "Class": "responsibilities"}, ignore_index=True)

for x in df['requirements']:
  df_1 = df_1.append({"Text": x, "Class": "requirements"}, ignore_index=True)

for x in df['terms']:
  df_1 = df_1.append({"Text": x, "Class": "terms"}, ignore_index=True)

for x in df['notes']:
  df_1 = df_1.append({"Text": x, "Class": "notes"}, ignore_index=True)

df_1 = df_1.dropna(subset=['Text'])

df_1 = df_1.reset_index()
df_1 = df_1.drop(['index'], axis=1)

def remove_stickers(text):
  if text != text:
    return np.nan
  # Паттерн для поиска стикеров в формате [стикер]
  pattern = r"[^\w.!?,:;$\s/\"\']*"
  cleaned_text = re.sub(pattern, "", str(text))
  return cleaned_text

for i in range(df_1.shape[0]):
  df_1['Text'][i] = remove_stickers(str(df_1['Text'][i]))

import string
def remove_punctuation(text):
    return "".join([ch if ch not in string.punctuation else ' ' for ch in text])

def remove_numbers(text):
    return ''.join([i if not i.isdigit() else ' ' for i in text])

import re
def remove_multiple_spaces(text):
	return re.sub(r'\s+', ' ', text, flags=re.I)

import nltk
nltk.download('stopwords')
from nltk.stem import *
from nltk.corpus import stopwords
from pymystem3 import Mystem
from string import punctuation
mystem = Mystem()

russian_stopwords = stopwords.words("russian")
russian_stopwords.extend(['…', '«', '»', '...'])
def lemmatize_text(text):
    tokens = mystem.lemmatize(text.lower())
    tokens = [token for token in tokens if token not in russian_stopwords and token != " "]
    text = " ".join(tokens)
    return text

preproccessing = lambda text: (remove_multiple_spaces(remove_numbers(remove_punctuation(text.lower()))))
df_1['Preproccessed'] = list(map(preproccessing, df_1['Text']))

from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer("russian")

russian_stopwords = stopwords.words("russian")
russian_stopwords.extend(['…', '«', '»', '...', 'т.д.', 'т', 'д'])

from nltk import word_tokenize
nltk.download('punkt')

stemmed_texts_list = []
for text in tqdm(df_1['Preproccessed']):
    tokens = word_tokenize(text)
    stemmed_tokens = [stemmer.stem(token) for token in tokens if token not in russian_stopwords]
    text = " ".join(stemmed_tokens)
    stemmed_texts_list.append(text)

df_1['text_stem'] = stemmed_texts_list

def remove_stop_words(text):
    tokens = word_tokenize(text)
    tokens = [token for token in tokens if token not in russian_stopwords and token != ' ']
    return " ".join(tokens)

sw_texts_list = []
for text in tqdm(df_1['Preproccessed']):
    tokens = word_tokenize(text)
    tokens = [token for token in tokens if token not in russian_stopwords and token != ' ']
    text = " ".join(tokens)
    sw_texts_list.append(text)

df_1['text_sw'] = sw_texts_list

lemm_texts_list = []
for text in tqdm(df_1['text_sw']):
    #print(text)
    try:
        text_lem = mystem.lemmatize(text)
        tokens = [token for token in text_lem if token != ' ' and token not in russian_stopwords]
        text = " ".join(tokens)
        lemm_texts_list.append(text)
    except Exception as e:
        print(e)

df_1['text_lemm'] = lemm_texts_list

def lemmatize_text(text):
    text_lem = mystem.lemmatize(text)
    tokens = [token for token in text_lem if token != ' ']
    return " ".join(tokens)

df_1[:10]

X = df_1['text_lemm']
#X = df_1['text_sw']
y = df_1['Class']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)

"""# Naive Bayes Classifier"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer

nb = Pipeline([('vect', CountVectorizer()),
               ('tfidf', TfidfTransformer()),
               ('clf', MultinomialNB()),
              ])

nb.fit(X_train, y_train)

from sklearn.metrics import classification_report
y_pred = nb.predict(X_test)

y_pred[0]

from sklearn.metrics import accuracy_score

classes = df_1['Class'].unique()

print('accuracy %s' % accuracy_score(y_pred, y_test))
print(classification_report(y_test, y_pred, target_names=classes))

text = ""
text = remove_multiple_spaces(remove_numbers(remove_punctuation(text.lower())))
text = remove_stop_words(text)
text = lemmatize_text(text)

pred = nb.predict([text])
pred

"""# Linear Support Vector Machine"""

from sklearn.linear_model import SGDClassifier

sgd = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),
               ])

sgd.fit(X_train, y_train)

y_pred = sgd.predict(X_test)
print('accuracy %s' % accuracy_score(y_pred, y_test))
print(classification_report(y_test, y_pred,target_names=classes))

text = "Авансирование дважды в месяц по 15 000 рублей, 15 и 30 числа"
text = remove_multiple_spaces(remove_numbers(remove_punctuation(text.lower())))
text = remove_stop_words(text)
text = lemmatize_text(text)

pred = sgd.predict([text])
pred

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression

logreg = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', LogisticRegression(n_jobs=1, C=1e5, max_iter=100)),
               ])

logreg.fit(X_train, y_train)

y_pred = logreg.predict(X_test)

print('accuracy %s' % accuracy_score(y_pred, y_test))
print(classification_report(y_test, y_pred,target_names=classes))

text = "Работа Маляр в Москве ПО 8 ЧАСОВ. Вакансия Маляр в строительстве на чистовую отделку больших объемов! Работа!  Открыт прием на несколько строительных объектов по Москве на выбор!  Большие ставки, готовы давать этажи бригадам!"
text = remove_multiple_spaces(remove_numbers(remove_punctuation(text.lower())))
text = remove_stop_words(text)
text = lemmatize_text(text)

pred = logreg.predict([text])
pred